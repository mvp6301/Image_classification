# -*- coding: utf-8 -*-
"""face_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ktLt5BEDsya_MzHiwaQqkErpes2nLSaA
"""

#from google.colab import drive
#drive.mount('/content/drive')



#!pip install keras-tuner

import os
import random
import cv2
import numpy as np
from imutils import paths
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Input, AveragePooling2D, Flatten, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from kerastuner.tuners import RandomSearch

#Define the path to your image dataset
dataset_path = "/content/drive/MyDrive/glasses-and-coverings/glasses-and-coverings"

#Get a list of image file paths
image_paths = list(paths.list_images(dataset_path))

# Shuffle the image paths
random.seed(42)
random.shuffle(image_paths)

# Initialize empty lists to store data and labels
data = []
labels = []

image_dims = (224, 224, 3)

for image_path in image_paths:
    # Load and resize the image
    image = cv2.imread(image_path)
    image = cv2.resize(image, (image_dims[1], image_dims[0]))

    # Preprocess the image
    image = image.astype("float") / 255.0

    # Append the image data to the 'data' list
    data.append(image)

    # Extract labels from the image path (modify this based on your dataset structure)
    label = image_path.split(os.path.sep)[-2].split("_")
    labels.append(label)

# Convert 'data' and 'labels' to NumPy arrays
data = np.array(data, dtype="float32")
labels = np.array(labels)

print(labels)

mlb = MultiLabelBinarizer()
labels = mlb.fit_transform(labels)

# Split the data into training and testing sets (before tuning)
trainX_before, testX_before, trainY_before, testY_before = train_test_split(data, labels, test_size=0.20)

# Define and compile the model before tuning
model_before = Sequential()
model_before.add(MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=image_dims)))
model_before.add(AveragePooling2D(pool_size=(2, 2)))
model_before.add(Flatten())
model_before.add(Dense(units=128, activation="relu"))  # You can change the number of units if needed
model_before.add(Dropout(rate=0.5))  # You can change the dropout rate if needed
model_before.add(Dense(units=4, activation='softmax'))
optimizer_before = Adam(learning_rate=1e-3)
model_before.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=optimizer_before)

# Train the model before tuning
model_before.fit(trainX_before, trainY_before, epochs=30, validation_data=(testX_before, testY_before))
# Evaluate the model before tuning
loss_before, accuracy_before = model_before.evaluate(testX_before, testY_before)
print("Before Tuning - Test loss:", loss_before)
print("Before Tuning - Test accuracy:", accuracy_before)

loss_before, accuracy_before = model_before.evaluate(trainX_before, trainY_before)
print("Before Tuning - Train loss:", loss_before)
print("Before Tuning - Train accuracy:", accuracy_before)

# Save the model before tuning
model_before.save("best_model_MobileNetV2_before_tuning.h5")

# Define a function to create the model
def build_model(hp):
    baseModel = MobileNetV2(weights='imagenet', include_top=False, input_tensor=Input(shape=image_dims))
    for layer in baseModel.layers[:-4]:
        layer.trainable = False

    model = Sequential()
    model.add(baseModel)
    model.add(AveragePooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    # Hyperparameter: Number of units in the first fully connected layer
    model.add(Dense(units=hp.Int('units_fc1', min_value=32, max_value=512, step=32), activation="relu"))
    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)))

    model.add(Dense(units=4, activation='softmax'))

    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]))
    model.compile(loss="categorical_crossentropy", metrics=["accuracy"], optimizer=optimizer)

    return model



# Split the data into training and testing sets
trainX_after, testX_after, trainY_after, testY_after = train_test_split(data, labels, test_size=0.20)

# Define the Keras Tuner RandomSearch tuner
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,  # Adjust the number of trials as needed
    directory='keras_tuner_results',
    project_name='glasses_and_coverings'
)

# Search for the best hyperparameters
tuner.search(trainX_after, trainY_after, epochs=30, validation_data=(testX_after, testY_after))

# Get the best model and hyperparameters
best_model = tuner.get_best_models(num_models=1)[0]
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

loss_hp, accuracy_after_hp = best_model.evaluate(testX_after, testY_after)
print("Test loss:", loss_hp)
print("Test accuracy_after:", accuracy_after_hp)

# Save the best model
best_model.save("bbest_model_MobileNetV2_with_tuning.h5")

# Print the best hyperparameters
print("Best hyperparameters:")
print(best_hyperparameters.get_config())

loss, accuracy = best_model.evaluate(trainX_after, trainY_after)
print("train loss:", loss)
print("train accuracy:", accuracy)

predictions_before = model_before.predict(testX_before)
predicted_labels_before = np.argmax(predictions_before, axis=1)
true_labels_before = np.argmax(testY_before, axis=1)
classification_report_before = classification_report(true_labels_before, predicted_labels_before, target_names=mlb.classes_)
with open("classification_report_before_tuning.txt", "w") as f:
    f.write(classification_report_before)

predictions_after = best_model.predict(testX_after)
predicted_labels_after = np.argmax(predictions_after, axis=1)
true_labels_after = np.argmax(testY_after, axis=1)
classification_report_after = classification_report(true_labels_after, predicted_labels_after, target_names=mlb.classes_)
with open("classification_report_after_tuning.txt", "w") as f:
    f.write(classification_report_after)

#Save the accuracy scores
print(f"Before Tuning - Test accuracy: {accuracy_before}")
print(f"After Tuning - Test accuracy: {accuracy_after_hp}")
with open("accuracy_scores.txt", "w") as f:
    f.write(f"Before Tuning - Test accuracy: {accuracy_before}\n")
    f.write(f"After Tuning - Test accuracy: {accuracy_after_hp}")


import tensorflow as tf
from keras.models import load_model
import os

# Load the best model (before or after tuning)
loaded_model = load_model("/content/bbest_model_MobileNetV2_with_tuning.h5")

# Convert the model to a quantized format (int8)
converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

# Define the path to your image dataset
dataset_path = "/content/drive/MyDrive/glasses-and-coverings/glasses-and-coverings"

# Get a list of image file paths in the dataset directory
image_paths = [os.path.join(dataset_path, filename) for filename in os.listdir(dataset_path) if filename.endswith((".jpg", ".jpeg", ".png"))]

# Provide a representative dataset for quantization
def representative_dataset():
    for image_path in image_paths:
        image = tf.io.read_file(image_path)
        image = tf.image.decode_image(image, channels=3)
        image = tf.image.resize(image, (224, 224))  # Adjust the size as needed
        image = tf.cast(image, tf.float32)
        image = image / 255.0  # Normalize to [0, 1]
        image = tf.expand_dims(image, axis=0)  # Add batch dimension
        yield [image]

converter.representative_dataset = representative_dataset

# Convert the model to a quantized TFLite model
quantized_tflite_model = converter.convert()

# Save the quantized model to a file
model_file_path = 'quantized_model.tflite'
with open(model_file_path, "wb") as f:
    f.write(quantized_tflite_model)

# Check the file size in bytes
model_file_size_bytes = os.path.getsize(model_file_path)

# Convert bytes to megabytes (MB) for a more human-readable size
model_file_size_mb = model_file_size_bytes / (1024 * 1024)

print(f"Quantized model size: {model_file_size_mb:.2f} MB")

#! pip install onnx
#! pip install tf2onnx
import onnx
import tf2onnx
import tensorflow as tf
from keras.models import load_model

# Load your TensorFlow/Keras model (best_model in your case)
loaded_model = load_model("/content/bbest_model_MobileNetV2_with_tuning.h5")

# Convert the model to ONNX format
onnx_model, _ = tf2onnx.convert.from_keras(loaded_model)

# Save the ONNX model to a file
onnx.save_model(onnx_model, "model.onnx")

